\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath} 
\usepackage{float}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{lineno}
\usepackage{lmodern}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{systeme}
\usepackage{amsmath,amssymb,amsthm,mathrsfs,latexsym,tikz,url}
\usepackage{epigraph,graphicx}
%\usepackage{titlesec} %For formatting sections
\usepackage{listings}
\usepackage{listingsutf8}
\usepackage{color}
\usepackage{todonotes}
\presetkeys%
    {todonotes}%
    {inline,backgroundcolor=yellow}{}

\graphicspath{ {./}{./figures/} {images/}}
\DeclareGraphicsExtensions{.png,.pdf}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=4
}


\setlength{\parindent}{0.0cm}
\setlength{\parskip}{0.1cm}


\begin{document}

\title{Lab 3 Machine Learning}
\author{Anton Str√•hle \& Jan Alexandersson}
\maketitle

\section*{Assignment 1}

In the function \texttt{mlParams} below we evaluate the $C \times d$ array $\boldsymbol\mu$ and the $C \times d \times d$ array $\boldsymbol\Sigma$ made up of the mean vectors $\boldsymbol\mu_k$ and the covariance matrices $\boldsymbol\Sigma_k$. The expressions for the mean vectors and the covariance matrices are derived through likelihood methods from bivariate guassian distributions.

\begin{lstlisting}
def mlParams(X, labels, W=None):
    assert(X.shape[0]==labels.shape[0])
    Npts,Ndims = np.shape(X)
    classes = np.unique(labels)
    Nclasses = np.size(classes)

    if W is None:
        W = np.ones((Npts,1))/float(Npts)

    mu = np.zeros((Nclasses,Ndims))
    sigma = np.zeros((Nclasses,Ndims,Ndims))
    
    for k in range(Nclasses):
        
        ind = np.where(labels == classes[k])[0]
        
        mu[k] = np.sum(X[ind,]*W[ind], axis = 0)/np.sum(W[ind])
        
        sigma[k] = np.diag(np.sum(W[ind]*np.power(X-mu[k],2)[ind],
        axis = 0)/np.sum(W[ind]))   
                          
    return mu, sigma
\end{lstlisting}

We can apply the function \texttt{mlParams} for data consisting of several bivariate gaussian distributions and then plot the corresponding $95\%$ confidence intervals for the respective distributions.

\begin{figure}
    \centering
    \includegraphics[height=100mm, width=100mm]{Figure_1.png}
    \caption{Guassian Confidence Intervals}
\end{figure}


\section*{Assignment 2}

In the function \texttt{computePrior} below we calculate the prior probabilities for each of the respective labels. 

\begin{lstlisting}
def computePrior(labels, W=None):
    Npts = labels.shape[0]
    if W is None:
        W = np.ones((Npts,1))/Npts
    else:
        assert(W.shape[0] == Npts)
    classes = np.unique(labels)
    Nclasses = np.size(classes)

    prior = np.zeros((Nclasses,1))
    
    for k in range(Nclasses):
        
        ind = np.where(labels == classes[k])[0]
        
        prior[k] = np.sum(W[ind])/np.sum(W)

    return prior
\end{lstlisting}

Using the previous function \texttt{mlParams} to compute the arrays $\boldsymbol\mu$ and $\boldsymbol\Sigma$ we can use \texttt{computePrior} to evaluate the posterior probabilities in the function \texttt{classifyBayes} which uses a naive bayes classifier to classify a set of points \texttt{X}.

\begin{lstlisting}
def classifyBayes(X, prior, mu, sigma):

    Npts = X.shape[0]
    Nclasses,Ndims = np.shape(mu)
    logProb = np.zeros((Nclasses, Npts))
    
    for k in range(Nclasses):
        
        diff = X-mu[k]
        
        logSigma = np.log(np.linalg.det(sigma[k]))/2
        
        logPrior = np.log(prior[k])
        
        for j in range(Npts):
            
            logProb[k][j] = -logSigma - np.inner(diff[j]/np.diag(sigma[k]), diff[j])/2 + logPrior

    h = np.argmax(logProb,axis=0)
    return h
\end{lstlisting}

\section*{Assignment 3}

We can use the aforementioned function \texttt{classifyBayes} to test how well it performs on separate data sets. The data sets that we have available are one containing data of different types of irises and one contains data for different vowels. 

We can plot the classification boundaries for the respective data sets as follows.

\begin{figure}
    \centering
    \includegraphics[scale = 0.90]{BayesIris.png}
    \caption{Bayes Classifier: Iris}
t\end{figure}

When using the naive bayes classifier for the iris dataset we get that the trained classifier has a mean classification accuracy of 89 with a standard deviation of 4.16.

\begin{figure}
    \centering
    \includegraphics[scale = 0.90]{BayesVowel.png}
    \caption{Bayes Classifier: Vowel}
\end{figure}

When using the naive bayes classifier for the vowel dataset we get that the trained classifier has a mean classification accuracy of 64.7 with a standard deviation of 4.03. It is not unexpected that the classifier has a higher accuracy for the iris data set compared to the vowel data set since the number of valid targets are much lower for the iris dataset, specifically 3 compared to 11.

Feature independence, which is assumed in the naive bayes classifier, is independence between the features conditioned on the class. In Figure 2 we observe that the features represented on the $x$ and $y$-axis could be said to be independent conditioned on Class 0. If we however observe Classes 1 and 2 instead we note that there is a clear linear dependancy between the features, meaning that the assumption is not satisfied. 

In the iris dataset presented in Figure 2 we note that Classes 1 and 2 seem to overlap slightly. One way to deal with the data, making it more sperabale, could be to transform it into higher dimension. It could also be possible to boost the algorithm, as will be done later on, in order to improve the boundary. 

\section*{Assignment 4 \& 5}

The rewritten versions of \texttt{mlParams} and \texttt{computePrior} were presented in Assignment 1.

We use these rewritten functions in order to implement the Adaboost algorithm as can be seen below.

The function \texttt{trainBoost} trains the booster.

\begin{lstlisting}
def trainBoost(base_classifier, X, labels, T=10):
    # these will come in handy later on
    Npts,Ndims = np.shape(X)
    
    classes = np.unique(labels)
    
    classifiers = [] # append new classifiers to this list
    alphas = [] # append the vote weight of the classifiers to this list

    # The weights for the first iteration
    wCur = np.ones((Npts,1))/float(Npts)

    for i_iter in range(0, T):
        # a new classifier can be trained like this, given the current weights
        classifiers.append(base_classifier.trainClassifier(X, labels, wCur))

        # do classification for each point
        vote = classifiers[-1].classify(X)
        
        eps = 0.0
        
        for k in classes:
            
            ind = np.where(vote == k)[0]
            
            eps += np.sum(np.transpose(wCur[ind])* (1-(k == labels[ind]))) 
        
        alpha = (np.log(1-eps) - np.log(eps))/2
        
        alphas.append(alpha)
        
        wOld = wCur
        
        for i in range(len(wOld)):
            
            wCur[i] = wOld[i]*np.exp(alpha * np.power(-1,(vote[i] == labels[i])))
        
        wCur = wCur/np.sum(wCur)
        
    return classifiers, alphas
\end{lstlisting}

Whilst the function \texttt{classifyBoost} classifies using the trained booster.

\begin{lstlisting}
def classifyBoost(X, classifiers, alphas, Nclasses):
    Npts = X.shape[0]
    Ncomps = len(classifiers)

    # if we only have one classifier, we may just classify directly
    if Ncomps == 1:
        return classifiers[0].classify(X)
    else:
        votes = np.zeros((Npts,Nclasses))
        
        for j in range(Ncomps):
            
            vote = classifiers[j].classify(X)
            
            for i in range(Npts):
                
                votes[i, vote[i]] += alphas[j]
            
        return np.argmax(votes,axis=1)
\end{lstlisting}

When using a boosted naive bayes classifiers we get the following boundary.
        
\begin{figure}
    \centering
    \includegraphics[scale = 0.90]{BoostedBayesIrirs.png}
    \caption{Adaboosted Bayes Classifier: Iris}
\end{figure}

We note that the decision boundry is a lote more complex compared to the non-boosted boundry presented in Figure 2. We also get a slight increase in mean classification accuracy 94.1 and with standard deviation 6.72 which is an increase from the non-boosted version. This increase is accuaracy is in some sense expected since we combine the classifications of the type used to classify in the first place. 

We can make up for the fact that the naive bayes classifier lacks complexity by using booisting since the boosting of weak classifiers tend to converge to stronger classifiers.

\section*{Assignment 6}






\end{document}
